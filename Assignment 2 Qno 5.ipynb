{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3edb35eb",
   "metadata": {},
   "source": [
    "Implement bagging method to train machine learning on SMS spam detection. \n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f60af5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "path = \"E:\\Milan\\MDS\\machine learning\\Dataset\\sms spam\\spam.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972f9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using utf-8 encoding to decode the byte data, errors= \"replace\" ignores bytes that cant be decoded\n",
    "with open(path,\"r\", encoding= \"utf-8\" , errors = \"replace\") as file: \n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a54e91e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokensy = word_tokenize(data)        #inserting each word in array\n",
    "stop_words = set(stopwords.words())  #set of unique words in tokensy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f13b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the string text in tokensy array\n",
    "for i in range(len(tokensy)):\n",
    "    a = tokensy[i]\n",
    "    if a ==\"||ham||\" or a ==\"||spam||\":        #\"||spam|| and ||ham|| are the categories so no need to normalize\"\n",
    "        pass\n",
    "    else:\n",
    "        tokensy[i] = a.lower()                 #changing word in array to lower case\n",
    "        a = tokensy[i]\n",
    "        tokensy[i]= re.sub(r'[^\\w]+',\" \",a)    #replacing any letter in words of array that is not alphanumeric, by a space character\n",
    "        tokensy[i] = re.sub(r'^[ ]+|[ ]+$',\"\",tokensy[i])  #removing space in words at either start or end of word\n",
    "        tokensy[i] = re.sub(r'(\\d+)|([a-zA-Z]+)|([ ]+)', r\"\\1\\2 \", tokensy[i])[:-1] #adding space betwwen alphabets and numbers if any\n",
    "        tokensy[i] = re.sub(r'[ ]+',r\" \",tokensy[i])  #removing multiple spaces in word with single space\n",
    "\n",
    "#removing the words from tokensy that is empty or are stop words\n",
    "tokensy = [item for item in tokensy if (item != \"\" and item not in stop_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c80e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = []\n",
    "x_data = []\n",
    "new_text = []\n",
    "#getting array of categorical words and assigning the words of that category to another array\n",
    "for i in range(len(tokensy)):\n",
    "    if tokensy[i].lower() == \"||ham||\" or tokensy[i].lower() == \"||spam||\":\n",
    "        y_data.append(tokensy[i])\n",
    "        x_data.append(new_text)\n",
    "        new_text= []\n",
    "    else:\n",
    "        new_text.append(tokensy[i])\n",
    "x_data.append(new_text)\n",
    "x_data = x_data[1:]                      #removing fist array of words since it is empty\n",
    "x_data[-1] = x_data[-1][:-2]             #removing headers of csv file from end index of array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad08bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining only outer elements of array to form string containing array of individual words so we could feed to CountVectorizer class of python\n",
    "change_dim = [\" \".join(x_data[i]) for i in range(len(x_data))]\n",
    "\n",
    "#Steps to create bag of words\n",
    "vectorizer = CountVectorizer(lowercase=False ) #false since it is already in lower case\n",
    "\n",
    "x = vectorizer.fit_transform(change_dim) #feeding string of individual words\n",
    "\n",
    "y_data = np.array(y_data)\n",
    "y_data[y_data == \"||ham||\"] = 1        #changing element of array that contains word \"||ham||\" to 1 as classifier\n",
    "y_data[y_data =='||spam||'] = 2        #changing element of array that contains word \"||spam||\" to 2 as classifier \n",
    "\n",
    "dense_matrix = x.toarray()             #changing to array\n",
    "table = pd.DataFrame(dense_matrix, columns = vectorizer.get_feature_names_out(), index = y_data)    #converting array to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8867b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting data to data frame\n",
    "x_data = pd.DataFrame(x_data)\n",
    "y_data = pd.DataFrame(y_data)\n",
    "y_data = y_data.applymap(lambda x: np.int64(x) if pd.notna(x) else x)  #converting all numbers that is in string data type to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12503afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingClassifier:\n",
    "    def __init__(self, base_classifier, n_estimators):        #constructor for the python class\n",
    "        self.base_classifier = base_classifier\n",
    "        self.n_estimators = n_estimators\n",
    "        self.classifiers = []\n",
    "        self.predictions = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_estimators):\n",
    "            #Bootstrap sampling with replacement having same length as variable X\n",
    "            indices = np.random.choice(len(X), len(X), replace=True)  #array of numbers from 0 to len(X) with total number of elements len(X)\n",
    "            X_sampled = X.iloc[indices]     #getting only data that match provided indices \n",
    "            y_sampled = y.iloc[indices]\n",
    "\n",
    "            # Create a new base classifier and train it on the sampled data\n",
    "            classifier = self.base_classifier.__class__()      #self.base_classifier is \"decision tree\"\n",
    "            classifier.fit(X_sampled, y_sampled)               #fit sampled data in decision tree classifier\n",
    "\n",
    "            # Store the trained classifier in the list of classifiers\n",
    "            self.classifiers.append(classifier)                #creating array  of decission tree classifier with different inputs\n",
    "            print(f\"Successfully fitted {i+1} base classifier with random {len(indices)} data with replacement\")\n",
    "            \n",
    "        return self.classifiers\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using all the base classifiers\n",
    "        self.predictions = [classifier.predict(X) for classifier in self.classifiers]  #predicting with data using decission tree and creating array\n",
    "        \n",
    "        # using voting to determine the classifying class for each observation of dataset\n",
    "        #bincount determines data with max frequency along given axis (i.e along column for this case)\n",
    "        #agrmax finds the index having max value data\n",
    "        majority_votes = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=self.predictions) \n",
    "        return majority_votes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fb792fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fitted 1 base classifier with random 3900 data with replacement\n",
      "Successfully fitted 2 base classifier with random 3900 data with replacement\n",
      "Successfully fitted 3 base classifier with random 3900 data with replacement\n",
      "Successfully fitted 4 base classifier with random 3900 data with replacement\n",
      "Successfully fitted 5 base classifier with random 3900 data with replacement\n",
      "\n",
      "Accuracy of bagging classifier for test data: 0.9706937799043063\n",
      "Accuracy of bagging classifier for train data: 0.9946153846153846\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(table, y_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create the base classifier\n",
    "dc = DecisionTreeClassifier()\n",
    "\n",
    "#assigning decission tree object as base classifier and no of different classifier\n",
    "model = BaggingClassifier(base_classifier=dc, n_estimators=5)  \n",
    "classifiers = model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy of test dat\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy of bagging classifier for test data:\", accuracy)\n",
    "\n",
    "#calculate the accuracy of training data\n",
    "y_pred = model.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(\"Accuracy of bagging classifier for train data:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40aeeba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of base classifier for test data 1 is: 0.9593301435406698\n",
      "Accuracy of base classifier for test data 2 is: 0.9671052631578947\n",
      "Accuracy of base classifier for test data 3 is: 0.965311004784689\n",
      "Accuracy of base classifier for test data 4 is: 0.9712918660287081\n",
      "Accuracy of base classifier for test data 5 is: 0.9635167464114832\n"
     ]
    }
   ],
   "source": [
    "for i, clf in enumerate(classifiers):\n",
    "\ty_pred = clf.predict(X_test)\n",
    "\t# Calculate accuracy\n",
    "\taccuracy = accuracy_score(y_test, y_pred)\n",
    "\tprint(\"Accuracy of base classifier for test data \"+str(i+1),'is:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d33cbf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of base classifier for train data 1 is: 0.9861538461538462\n",
      "Accuracy of base classifier for train data 2 is: 0.9851282051282051\n",
      "Accuracy of base classifier for train data 3 is: 0.9851282051282051\n",
      "Accuracy of base classifier for train data 4 is: 0.9841025641025641\n",
      "Accuracy of base classifier for train data 5 is: 0.9851282051282051\n"
     ]
    }
   ],
   "source": [
    "for i, clf in enumerate(classifiers):\n",
    "\ty_pred = clf.predict(X_train)\n",
    "\t# Calculate accuracy\n",
    "\taccuracy = accuracy_score(y_train, y_pred)\n",
    "\tprint(\"Accuracy of base classifier for train data \"+str(i+1),'is:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
